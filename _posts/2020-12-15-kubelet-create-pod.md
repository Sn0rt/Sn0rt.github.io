---
author: sn0rt
comments: true
date: 2020-12-15
layout: post
title: kubelet æ˜¯å¦‚ä½•å¯åŠ¨ POD 
tag: k8s
---

å‡ºäº `containerd` ä¸Šçº¿éœ€æ±‚ï¼Œèµ°è¯»çº¿ä¸Šç»„ä»¶çš„ä»£ç ç¡®å®šç°æœ‰ `POD` åˆ›å»ºæµç¨‹ï¼Œä¸»è¦å…³æ³¨çš„æ˜¯ç»„ä»¶ä¹‹é—´æ˜¯å¦‚ä½•äº¤äº’æ²Ÿé€šçš„è¿›è¡Œ `POD` åˆ›å»ºçš„ã€‚

![kubelet-create-pod](../media/pic/kubelet-create-pod-with-docker.jpg)

<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->

- [kubelet](#kubelet)
  - [kubelet å¦‚ä½•åˆ›å»º POD](#kubelet-%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA-pod)
- [dockershim](#dockershim)
- [dockerd æ¥å£](#dockerd-%E6%8E%A5%E5%8F%A3)
  - [/containers/{name:.*}/start](#containersnamestart)
- [containerd](#containerd)
  - [ContainersServer.Create æ–¹æ³•çš„å®ç°](#containersservercreate-%E6%96%B9%E6%B3%95%E7%9A%84%E5%AE%9E%E7%8E%B0)
  - [TasksServer.Create æ–¹æ³•çš„å®ç°](#tasksservercreate-%E6%96%B9%E6%B3%95%E7%9A%84%E5%AE%9E%E7%8E%B0)
  - [TasksServer.Start æ–¹æ³•çš„å®ç°](#tasksserverstart-%E6%96%B9%E6%B3%95%E7%9A%84%E5%AE%9E%E7%8E%B0)
- [containerd-shim çš„å¦‚ä½•å·¥ä½œ](#containerd-shim-%E7%9A%84%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C)
  - [contained-shim create](#contained-shim-create)
  - [contained-shim start](#contained-shim-start)
- [runc](#runc)
  - [run create --bundle](#run-create---bundle)
  - [runc init](#runc-init)
  - [runc start](#runc-start)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

# kubelet

çº¿ä¸Šä¸»è¦æœ‰ä¸¤ä¸ªç‰ˆæœ¬çš„ `k8s`ï¼Œè¿™æ¬¡ä»£ç èµ°è¯»ä¸»è¦å…³æ³¨ `1.17.4` ç‰ˆæœ¬ã€‚è§‚å¯Ÿ `kubelet` åˆå§‹åŒ–æµç¨‹å¯ä»¥çœ‹åˆ°ä¸è¿è¡Œæ—¶ç›¸å…³å‚æ•°å¦‚ä¸‹`--container-runtime=` å‚æ•°ä¸º `docker`ã€‚æŸ¥çœ‹å¯åŠ¨æ—¥å¿—ï¼Œå¯ä»¥çœ‹åˆ° `kubelet` å¯åŠ¨äº†ä¸€ä¸ª `dockershim` çš„æœåŠ¡ï¼Œè¿™ä¸ªæœåŠ¡å°±æ˜¯ç›®å‰è°ƒç ”çš„ä¸€ä¸ªå…³é”®ç‚¹ï¼Œå› ä¸ºç¤¾åŒºåœ¨ `1.20` ç‰ˆæœ¬ä¸­å‡†å¤‡å¼ƒç”¨ `dockershim` äº†ã€‚

```
I1215 16:38:23.359299 2052022 docker_service.go:274] Setting cgroupDriver to cgroupfs
I1215 16:38:23.359435 2052022 kubelet.go:642] Starting the GRPC server for the docker CRI shim.
I1215 16:38:23.359456 2052022 docker_server.go:59] Start dockershim grpc server
...
```

è°ƒç ”ç¤¾åŒºæ–‡æ¡£å‘ç°ï¼Œ`dockershim` ä¹‹æ‰€ä»¥è¢«æå‡ºæ˜¯ä¸ºäº†è§£å†³ `kubernetes` å¼€å‘è€…é¢ä¸´å¤šä¸ª`runtime`éƒ½è¦æ¥å…¥`kubernetes`å¯¼è‡´è°ƒç”¨è¿è¡Œæ—¶çš„ç›¸å…³ä»£ç æ¥å£ä¸ç¨³å®šçš„é—®é¢˜ã€‚
å¼€å‘è€…å¼•å…¥ä¸€ä¸ªæŠ½è±¡å±‚å¯¹ä¸Šå±è”½åº•å±‚çš„`runtime`å®ç°å·®å¼‚ï¼Œè¿™ä¸ªæŠ½è±¡å±‚ç§°ä¸º[CRI]([https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/)), è¿™é‡Œçš„ `dockershim` å°±æ˜¯åŸºäº `docker` äºŒæ¬¡å°è£…çš„ä¸€ä¸ª `CRI` å®ç°ã€‚
`shim` è¿™ä¸ªå•è¯çš„ç”±æ¥å¯ä»¥ä» wiki ä¸ŠæŸ¥åˆ° `an application compatibility workaround`ã€‚

## kubelet å¦‚ä½•åˆ›å»º POD

åœ¨ `POD` åˆ›å»ºè¿‡ç¨‹ä¸­é¦–å…ˆéœ€è¦å‡†å¤‡ä¸€ä¸ªè¢«æˆä¸º `sandbox` çš„å®¹å™¨ç”¨æ¥åˆå§‹åŒ–è®¾ç½®åŸºç¡€ç½‘ç»œç­‰ç­‰è¯¸å¤šäº‹å®œã€‚èµ°è¯» `CRI` æ¥å£å‘ç°åˆ›å»º `Sandbox` è¿™ä¸ªæ˜¯ `CRI` çš„ä¸€ä¸ªé‡è¦åŠŸèƒ½ã€‚

åŸºäº `kubelet` çš„æºç åˆ†æï¼Œé€šè¿‡æºç åˆ†æäº†è§£åˆ° `syncLoopIteration` -> `HandlePodAdditions` -> `dispatchWork`->`UpdatePod`-> `managePodLoop` è¿™æ ·çš„ä¸€ä¸ªå‡½æ•°è°ƒç”¨å…³ç³»ã€‚

```go
func (p *podWorkers) managePodLoop(podUpdates <-chan UpdatePodOptions) {
	var lastSyncTime time.Time
...
			err = p.syncPodFn(syncPodOptions{ // å°±æ˜¯è¿™ä¸ª syncPodFn å‡½æ•°çš„å®ç°
				mirrorPod:      update.MirrorPod,
				pod:            update.Pod,
				podStatus:      status,
				killPodOptions: update.KillPodOptions,
				updateType:     update.UpdateType,
			})
```

å…¶ä¸­çš„ `syncPodFn` å…·ä½“å®ç°æ˜¯ `klet.syncPod` å‡½æ•°ï¼Œå¯ä»¥é€šè¿‡ä»£ç `klet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache)`çœ‹å‡ºã€‚åŸºäºä»¥ä¸Šä¿¡æ¯å¯ä»¥æ¨è®ºå‡º `podWorkers` æ‰æ˜¯ `pod` åˆ›å»ºçš„æ ¸å¿ƒã€‚èµ°è¯»`woker`çš„ `SyncPod` è¿™ä¸ªå‡½æ•°å°±æ˜¯å°±æ˜¯ `podWorkers` æ ¸å¿ƒæµç¨‹äº†ï¼Œ`worker` é€šè¿‡ `sync` å‡½æ•°æ¥è°ƒç”¨å„ä¸ª `CRI` æ¥å£å°†è¿™äº›åŸå­æ¥å£ç»„è£…æˆä¸€ä¸ªä¸ªå®é™…çš„ `kubernetes` çš„ `POD`ã€‚

```go
// SyncPod syncs the running pod into the desired pod by executing following steps:
//
//  1. Compute sandbox and container changes.
//  2. Kill pod sandbox if necessary.
//  3. Kill any containers that should not be running.
//  4. Create sandbox if necessary.
//  5. Create ephemeral containers.
//  6. Create init containers.
//  7. Create normal containers.
func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) {
	// Step 1: Compute sandbox and container changes.
	podContainerChanges := m.computePodActions(pod, podStatus) // è¿˜æ˜¯å£°æ˜å¼apiæ¨¡å‹ï¼Œå°†æ“ä½œåºåˆ—åŒ–
...
	// Step 2: Kill the pod if the sandbox has changed.
	if podContainerChanges.KillPod {
...
		killResult := m.killPodWithSyncResult(pod, kubecontainer.ConvertPodStatusToRunningPod(m.runtimeName, podStatus), nil)
... // åˆ é™¤ä¸€ä¸‹æ— å…³ç´§è¦çš„ä»£ç ï¼Œä¸å½±å“ä¸»çº¿é€»è¾‘
	} else {
		// Step 3: kill any running containers in this pod which are not to keep.
		for containerID, containerInfo := range podContainerChanges.ContainersToKill {
			klog.V(3).Infof("Killing unwanted container %q(id=%q) for pod %q", containerInfo.name, containerID, format.Pod(pod))
			killContainerResult := kubecontainer.NewSyncResult(kubecontainer.KillContainer, containerInfo.name)
			result.AddSyncResult(killContainerResult)
			if err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil {
...// ä¸€äº›åœ¨è¿è¡Œï¼Œä½†æ˜¯åº”è¯¥è¢«åˆ é™¤çš„ container å¤„ç†é€»è¾‘ï¼Œ
			}
		}
	}
...
  // Step 4: Create a sandbox for the pod if necessary.
	podSandboxID := podContainerChanges.SandboxID
	if podContainerChanges.CreateSandbox {
...
    createSandboxResult := kubecontainer.NewSyncResult(kubecontainer.CreatePodSandbox, format.Pod(pod))
		result.AddSyncResult(createSandboxResult)
		podSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt) 
    // è¿™ä¸ªå‡½æ•°æ˜¯æˆ‘ä»¬å…³æ³¨çš„æ ¸å¿ƒï¼Œå‡†å¤‡podçš„æ²™ç›’ï¼Œå¦‚ä½•åœ¨ sandbox ä¸­è°ƒç”¨ CNI è®¾ç½®ç½‘ç»œ/å­˜å‚¨ç­‰
... // åˆ é™¤ä¸€ä¸‹é”™è¯¯å¤„ç†ï¼Œäº‹ä»¶çš„é€»è¾‘ï¼Œç½‘ç»œç›¸å…³ï¼Œä¸å½±å“å¯åŠ¨ä¸€ä¸ªä¸ä½¿ç”¨ç½‘ç»œçš„ container ğŸ˜„
 
	// Get podSandboxConfig for containers to start.
	configPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID)
	result.AddSyncResult(configPodSandboxResult)
	podSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt)
	if err != nil {
		message := fmt.Sprintf("GeneratePodSandboxConfig for pod %q failed: %v", format.Pod(pod), err)
		klog.Error(message)
		configPodSandboxResult.Fail(kubecontainer.ErrConfigPodSandbox, message)
		return
	}

	// Helper containing boilerplate common to starting all types of containers.
	// typeName is a label used to describe this type of container in log messages,
	// currently: "container", "init container" or "ephemeral container"
	start := func(typeName string, container *v1.Container) error {
		startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, container.Name)
		result.AddSyncResult(startContainerResult)

		isInBackOff, msg, err := m.doBackOff(pod, container, podStatus, backOff)
		if isInBackOff {
			startContainerResult.Fail(err, msg)
			klog.V(4).Infof("Backing Off restarting %v %+v in pod %v", typeName, container, format.Pod(pod))
			return err
		}

		klog.V(4).Infof("Creating %v %+v in pod %v", typeName, container, format.Pod(pod))
		// NOTE (aramase) podIPs are populated for single stack and dual stack clusters. Send only podIPs.
		if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, podIPs); err != nil { // åˆ›å»º cotnainer å’Œ å¯åŠ¨ container æ˜¯ä¸¤å›äº‹ï¼Œåˆ›å»ºè¯´çš„æ˜¯å‡†å¤‡å¥½ç›¸å…³åº•å±‚æ–‡ä»¶èµ„æºï¼Œå¯åŠ¨å°±æ˜¯ä»¥è¿›ç¨‹å½¢å¼åœ¨OSå¯è§ï¼Œå…·ä½“è¿˜è¦çœ‹åº•å±‚è¿è¡Œæ—¶æ˜¯å¦‚ä½•å®ç°çš„ã€‚
      // .. åˆ é™¤äº†ä¸€éƒ¨åˆ†é”™è¯¯å¤„ç†çš„é€»è¾‘
      
	// Step 5: start ephemeral containers
	//  è¿™éƒ¨åˆ†é€»è¾‘éä¸»çº¿

	// Step 6: start the init container.
	if container := podContainerChanges.NextInitContainerToStart; container != nil {
		// Start the next init container.
		if err := start("init container", container); err != nil {
			return
		}

		// Successfully started the container; clear the entry in the failure
		klog.V(4).Infof("Completed init container %q for pod %q", container.Name, format.Pod(pod))
	}

	// Step 7: start containers in podContainerChanges.ContainersToStart. 
	for _, idx := range podContainerChanges.ContainersToStart { // å°±æ˜¯ä¸šåŠ¡çš„ container 
		start("container", &pod.Spec.Containers[idx]) // start å¯ä»¥å‰é¢çœ‹åˆ°å®šä¹‰ start := func(typeName string, container *v1.Container) 
	}

	return
}
```

ä¸Šé¢çš„æºç å…¶å®å·²ç»çŸ¥é“ä¸ºä»€ä¹ˆè¦å¼•å…¥ `sandbox container` äº†ã€‚åˆ†æä¸€ä¸‹ `createPodSandbox` è¿™ä¸ªå‡½æ•°æ˜¯å¦‚ä½•å®ç°çš„ã€‚

```go
// createPodSandbox creates a pod sandbox and returns (podSandBoxID, message, error).
func (m *kubeGenericRuntimeManager) createPodSandbox(pod *v1.Pod, attempt uint32) (string, string, error) {
	podSandboxConfig, err := m.generatePodSandboxConfig(pod, attempt)
	if err != nil {
		message := fmt.Sprintf("GeneratePodSandboxConfig for pod %q failed: %v", format.Pod(pod), err)
		klog.Error(message)
		return "", message, err
	}

	// Create pod logs directory
	err = m.osInterface.MkdirAll(podSandboxConfig.LogDirectory, 0755) // è¿™ä¸ªç›®å½•å°±æ˜¯ kubelet é…ç½®è¯»å–ä¸‹æ¥çš„
	if err != nil {
		message := fmt.Sprintf("Create pod log directory for pod %q failed: %v", format.Pod(pod), err)
... // çœç•¥é”™è¯¯å¤„ç†	
... // åˆ é™¤äº†éä¸»çº¿é€»è¾‘
	podSandBoxID, err := m.runtimeService.RunPodSandbox(podSandboxConfig, runtimeHandler)
... // çœç•¥é”™è¯¯å¤„ç†
	return podSandBoxID, "", nil
}
```

è¿™å°±æ˜¯çœ‹åˆ°è°ƒç”¨çš„ `func (in instrumentedRuntimeService) RunPodSandboxha` å‡½æ•°åŸºæœ¬æ˜¯é€ä¼ ï¼Œå­˜ç²¹æ˜¯ä¸ºäº†è®°å½•ä¸€ä¸‹ `metric` ä¸ºç›‘æ§æœåŠ¡ã€‚

```go
func (in instrumentedRuntimeService) RunPodSandbox(config *runtimeapi.PodSandboxConfig, runtimeHandler string) (string, error) {
	const operation = "run_podsandbox"
	startTime := time.Now()
	defer recordOperation(operation, startTime)
	defer metrics.RunPodSandboxDuration.WithLabelValues(runtimeHandler).Observe(metrics.SinceInSeconds(startTime))

	out, err := in.service.RunPodSandbox(config, runtimeHandler)
	recordError(operation, err)
	if err != nil {
		metrics.RunPodSandboxErrors.WithLabelValues(runtimeHandler).Inc()
...
```

åœ¨ä¸Šé¢è®°å½•å®Œæˆç›‘æ§ç›¸å…³è¿‡åï¼Œä¸‹é¢å°±æ˜¯å‡†å¤‡è°ƒç”¨ `grpc` æœåŠ¡äº†ï¼Œå¯ä»¥çœ‹åˆ°è°ƒç”¨çš„æ˜¯ `CRI` çš„ `/runtime.v1alpha2.RuntimeService/RunPodSandbox` æ¥å£ã€‚

```go
// RunPodSandbox creates and starts a pod-level sandbox. Runtimes should ensure
// the sandbox is in ready state.
func (r *RemoteRuntimeService) RunPodSandbox(config *runtimeapi.PodSandboxConfig, runtimeHandler string) (string, error) {
	// Use 2 times longer timeout for sandbox operation (4 mins by default)
	// TODO: Make the pod sandbox timeout configurable.
	ctx, cancel := getContextWithTimeout(r.timeout * 2)
	defer cancel()

	resp, err := r.runtimeClient.RunPodSandbox(ctx, &runtimeapi.RunPodSandboxRequest{
		Config:         config,
		RuntimeHandler: runtimeHandler,
	})
...
	return resp.PodSandboxId, nil
}
```

```go
func (c *runtimeServiceClient) RunPodSandbox(ctx context.Context, in *RunPodSandboxRequest, opts ...grpc.CallOption) (*RunPodSandboxResponse, error) {
	out := new(RunPodSandboxResponse)
	err := c.cc.Invoke(ctx, "/runtime.v1alpha2.RuntimeService/RunPodSandbox", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}
```

ä»¥ä¸Šå°±æ˜¯ `kubeGenericRuntimeManager` æ‰€å¤„ç†çš„ `POD` åˆ›å»ºæµç¨‹ä¸­çš„ `sandbox` çš„éƒ¨åˆ†ï¼Œå¯¹å…¶ `sandbox` å…·ä½“æ˜¯å¦‚ä½•è¢«åˆ›å»ºçš„æ›´å¤šç»†èŠ‚å°±è¦çœ‹ `grpc` æœåŠ¡ç«¯çš„å®ç°äº†ã€‚

# dockershim

å‰é¢æ¢³ç†å‡ºæ¥äº† `kubeGenericRuntimeManager` æ˜¯å¦‚ä½•é€šè¿‡`SyncPod`å®ç°`POD`çš„åˆ›å»ºï¼Œä½†æ˜¯ä¹Ÿå‘ç°ä»–çš„å®ç°å®Œå…¨æ˜¯é¢å‘æ¥å£çš„ï¼Œå®Œå…¨ä¸å…³å¿ƒåº•å±‚æ˜¯å¦‚ä½•å®ç° `cotnainer` çš„ã€‚æˆ‘ä»¬æ¥ç€ä¸Šé¢ `client` çš„ç›¸å…³é€»è¾‘èµ°è¯»ä¸€ä¸‹`/runtime.v1alpha2.RuntimeService/RunPodSandbox` çš„æœåŠ¡ç«¯ä»£ç é€»è¾‘ã€‚

```go
var _RuntimeService_serviceDesc = grpc.ServiceDesc{
	ServiceName: "runtime.v1alpha2.RuntimeService",
	HandlerType: (*RuntimeServiceServer)(nil),
	Methods: []grpc.MethodDesc{
...
		{
			MethodName: "RunPodSandbox",
			Handler:    _RuntimeService_RunPodSandbox_Handler,
...
```

ä¸‹é¢æ˜¯ `handler` æ³¨å†Œçš„è¿‡ç¨‹ï¼Œå…¶å®éƒ½æ˜¯æœºå™¨ç”Ÿæˆçš„ä»£ç æ²¡æœ‰å•¥å¥½çœ‹çš„ã€‚

```go
func _RuntimeService_RunPodSandbox_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(RunPodSandboxRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(RuntimeServiceServer).RunPodSandbox(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/runtime.v1alpha2.RuntimeService/RunPodSandbox",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(RuntimeServiceServer).RunPodSandbox(ctx, req.(*RunPodSandboxRequest))
```

è¿™é‡Œå°±æ˜¯ `dockershim` å®ç°çš„ `CRI`ï¼Œèµ°è¯»ä¸Šä¸‹æ–‡ä»£ç çŸ¥é“äº† `ds.client.StartContainer()` ä¸­çš„ `client` æ˜¯ `libdocker` ä¸­ï¼Œä¹Ÿå°±æ˜¯è°ƒç”¨çš„ `dockerClient`ã€‚

```go
// RunPodSandbox creates and starts a pod-level sandbox. Runtimes should ensure
// the sandbox is in ready state.
// For docker, PodSandbox is implemented by a container holding the network
// namespace for the pod.
// Note: docker doesn't use LogDirectory (yet).
func (ds *dockerService) RunPodSandbox(ctx context.Context, r *runtimeapi.RunPodSandboxRequest) (*runtimeapi.RunPodSandboxResponse, error) {
	config := r.GetConfig()

	// Step 1: Pull the image for the sandbox.
	image := defaultSandboxImage // è¿™é‡Œå°±æ˜¯ Google çš„é‚£ä¸ª sandbox çš„ image
	podSandboxImage := ds.podSandboxImage
	if len(podSandboxImage) != 0 {
		image = podSandboxImage
	}

	// NOTE: To use a custom sandbox image in a private repository, users need to configure the nodes with credentials properly.
	// see: http://kubernetes.io/docs/user-guide/images/#configuring-nodes-to-authenticate-to-a-private-repository
	// Only pull sandbox image when it's not present - v1.PullIfNotPresent.
	if err := ensureSandboxImageExists(ds.client, image); err != nil {
		return nil, err
	}

	// Step 2: Create the sandbox container.
	if r.GetRuntimeHandler() != "" && r.GetRuntimeHandler() != runtimeName {
		return nil, fmt.Errorf("RuntimeHandler %q not supported", r.GetRuntimeHandler())
	}
	createConfig, err := ds.makeSandboxDockerConfig(config, image)
	if err != nil {
		return nil, fmt.Errorf("failed to make sandbox docker config for pod %q: %v", config.Metadata.Name, err)
	}
	createResp, err := ds.client.CreateContainer(*createConfig) // è¿™é‡Œçš„åˆ›å»º sandbox æ˜¯å¤‡å¥½ container éœ€è¦åº•å±‚èµ„æº
	if err != nil {
		createResp, err = recoverFromCreationConflictIfNeeded(ds.client, *createConfig, err)
	}

	if err != nil || createResp == nil {
		return nil, fmt.Errorf("failed to create a sandbox for pod %q: %v", config.Metadata.Name, err)
	}
	resp := &runtimeapi.RunPodSandboxResponse{PodSandboxId: createResp.ID}

	ds.setNetworkReady(createResp.ID, false)
	defer func(e *error) {
		// Set networking ready depending on the error return of
		// the parent function
		if *e == nil {
			ds.setNetworkReady(createResp.ID, true)
		}
	}(&err)

	// Step 3: Create Sandbox Checkpoint.
	if err = ds.checkpointManager.CreateCheckpoint(createResp.ID, constructPodSandboxCheckpoint(config)); err != nil {
		return nil, err // ç›®å‰æ¥çœ‹ checkpoint åŠŸèƒ½å¥½åƒå¹¶æ²¡æœ‰å¤§è§„æ¨¡ä½¿ç”¨ï¼Ÿä¸è¿‡è¿™ä¸ªå¯¹æˆ‘ä»¬è¿™æ¬¡çœ‹ pod åˆ›å»ºæµç¨‹æ— å…³ç»éªŒ
	}

	// Step 4: Start the sandbox container.
	// Assume kubelet's garbage collector would remove the sandbox later, if
	// startContainer failed.
	err = ds.client.StartContainer(createResp.ID) // å‰é¢å‡†å¤‡å¥½ container è¿™é‡Œæ‰å»å¯åŠ¨ï¼Œå…·ä½“æ€ä¹ˆå¯åŠ¨ kubelet å¹¶ä¸å…³å¿ƒ
	if err != nil {
		return nil, fmt.Errorf("failed to start sandbox container for pod %q: %v", config.Metadata.Name, err)
	}

	// åˆ é™¤äº†ä¸€ä¸‹ç½‘ç»œå’Œå®‰å…¨ç›¸å…³ä»£ç å’Œæ ¸å¿ƒæµç¨‹æ— å…³

	// Step 5: Setup networking for the sandbox.
	// All pod networking is setup by a CNI plugin discovered at startup time.
	// This plugin assigns the pod ip, sets up routes inside the sandbox,
	// creates interfaces etc. In theory, its jurisdiction ends with pod
	// sandbox networking, but it might insert iptables rules or open ports
	// on the host as well, to satisfy parts of the pod spec that aren't
	// recognized by the CNI standard yet.
	cID := kubecontainer.BuildContainerID(runtimeName, createResp.ID)
	networkOptions := make(map[string]string)
	if dnsConfig := config.GetDnsConfig(); dnsConfig != nil {
		// Build DNS options.
		dnsOption, err := json.Marshal(dnsConfig)
		if err != nil {
			return nil, fmt.Errorf("failed to marshal dns config for pod %q: %v", config.Metadata.Name, err)
		}
		networkOptions["dns"] = string(dnsOption)
	}
  // CRI è°ƒç”¨ CNI æ¥è®¾ç½® POD çš„åŸºç¡€ç½‘ç»œ
	err = ds.network.SetUpPod(config.GetMetadata().Namespace, config.GetMetadata().Name, cID, config.Annotations, networkOptions)

  // åˆ é™¤äº†ä¸€éƒ¨åˆ†è®¾ç½®å®¹å™¨ç½‘ç»œé”™è¯¯çš„å¤„ç†æµç¨‹å’Œæ ¸å¿ƒæµç¨‹æ— å…³

	return resp, nil
}
```

èµ°è¯»çš„ `dockershim` çš„ `RunPodSandbox` æ¥å£å‘ç°ï¼Œå®ƒè°ƒç”¨äº† `docker` çš„æ¥å£ `ds.client.CreateContainer(*createConfig)` åˆ›å»ºäº† `cotnainer`, åˆä½¿ç”¨äº† `ds.client.StartContainer(createResp.ID)` å¯åŠ¨åˆšåˆšåˆ›å»ºçš„ `cotnainer`ã€‚åœ¨å¾€ä¸‹é¢å®ç°å°±éœ€è¦å»èµ°è¯» `dockerd` çš„ä»£ç äº†ã€‚

```go
func (ds *dockerService) RunPodSandbox(ctx context.Context, r *runtimeapi.RunPodSandboxRequest) (*runtimeapi.RunPodSandboxResponse, error) {
	config := r.GetConfig()
...
	createConfig, err := ds.makeSandboxDockerConfig(config, image)
	if err != nil {
		return nil, fmt.Errorf("failed to make sandbox docker config for pod %q: %v", config.Metadata.Name, err)
	}
	createResp, err := ds.client.CreateContainer(*createConfig) // è¿™é‡Œçš„åˆ›å»º sandbox æ˜¯å¤‡å¥½ container éœ€è¦åº•å±‚èµ„æº
...
	// Step 4: Start the sandbox container.
	// Assume kubelet's garbage collector would remove the sandbox later, if
	// startContainer failed.
	err = ds.client.StartContainer(createResp.ID) // å‰é¢å‡†å¤‡å¥½ container è¿™é‡Œæ‰å»å¯åŠ¨ï¼Œå…·ä½“æ€ä¹ˆå¯åŠ¨ kubelet å¹¶ä¸å…³å¿ƒ
	if err != nil {
		return nil, fmt.Errorf("failed to start sandbox container for pod %q: %v", config.Metadata.Name, err)
	}
```

è¿™é‡Œä¸»è¦å…³æ³¨ `ds.client.StartContainer()`ï¼Œæ˜¯å› ä¸º `ds.client.CreateContainer(*createConfig)` çš„é€»è¾‘æ¯”è¾ƒç®€å•ã€‚

```go
	resp, err := cli.post(ctx, "/containers/"+containerID+"/start", query, nil, nil)
```

é€šè¿‡å¯¹ä»£ç çš„åˆ†æå‘ç° `StartContainer` æœ€åå‘å‡ºçš„æ˜¯ `http` è¯·æ±‚ã€‚

# dockerd æ¥å£

##  /containers/{name:.*}/start

åŸºäºå‰é¢çš„åˆ†ææˆ‘å·²ç»çŸ¥é“ `kubelet` çš„ `dockershim` å…¶å®ä½¿ç”¨è°ƒç”¨çš„ `docker` çš„ `resftul api`ï¼Œæ ¹æ®çº¿ä¸Š `docker` çš„ç‰ˆæœ¬ä¿¡æ¯ `18.09.9` `checkout`ç›¸å¯¹åº”çš„ä»£ç ã€‚å¯ä»¥å‘ç° `container` æµç¨‹ç›¸å…³çš„é€»è¾‘åœ¨ `docker/api/server/router/container/container.go` çš„ `initRoutes()` ä¸­ã€‚

```go
		router.NewPostRoute("/containers/{name:.*}/start", r.postContainersStart),
```

ä¸Šé¢æ˜¯ handler ä¸»è¦é€»è¾‘å¾ˆç®€å•ï¼Œä¸‹é¢å°±æ˜¯å¯åŠ¨ `container` çš„æ ¸å¿ƒæµç¨‹äº†ã€‚

```go
func (s *containerRouter) postContainersStart(ctx context.Context, w http.ResponseWriter, r *http.Request, vars map[string]string) error {
	// If contentLength is -1, we can assumed chunked encoding
	// or more technically that the length is unknown
	// https://golang.org/src/pkg/net/http/request.go#L139
	// net/http otherwise seems to swallow any headers related to chunked encoding
	// including r.TransferEncoding
	// allow a nil body for backwards compatibility

	// ç§»é™¤ http æ£€æŸ¥ç›¸å…³é€»è¾‘ï¼Œå“ªäº›ä¸å½±å“æ ¸å¿ƒ
	if err := s.backend.ContainerStart(vars["name"], hostConfig, checkpoint, checkpointDir); err != nil { // here 
...
```

```go
// ContainerStart starts a container.
func (daemon *Daemon) ContainerStart(name string, hostConfig *containertypes.HostConfig, checkpoint string, checkpointDir string) error {
// åˆ é™¤å¤§é‡çš„ä¿æŠ¤å¼ç¼–ç¨‹éæ ¸å¿ƒé€»è¾‘ä»£ç 
	return daemon.containerStart(container, checkpoint, checkpointDir, true) // here
}
```

è·Ÿå¦‚è¿™ä¸ª`Start`å‡½æ•°å‘ç°å±…ç„¶æ˜¯ä½¿ç”¨ `grpc `å»è°ƒç”¨`/containerd.services.tasks.v1.Tasks/Start`çš„æ¥å£ã€‚

```go
// containerStart prepares the container to run by setting up everything the
// container needs, such as storage and networking, as well as links
// between containers. The container is left waiting for a signal to
// begin running.
func (daemon *Daemon) containerStart(container *container.Container, checkpoint string, checkpointDir string, resetRestartManager bool) (err error) {
	start := time.Now()
	container.Lock()
	defer container.Unlock()
	// åˆ é™¤é”™è¯¯å¤„ç†ä¸ä¸€ä¸‹é˜²å¾¡ç¼–ç¨‹

	if err := daemon.conditionalMountOnStart(container); err != nil {
		return err
	}

	if err := daemon.initializeNetworking(container); err != nil {
		return err
	}

	spec, err := daemon.createSpec(container)
	if err != nil {
		return errdefs.System(err)
	}

// åˆ é™¤ä¸€ä¸‹éæ ¸å¿ƒåŠŸèƒ½ä»£ç 

	createOptions, err := daemon.getLibcontainerdCreateOptions(container)
	if err != nil {
		return err
	}

	ctx := context.TODO()

	err = daemon.containerd.Create(ctx, container.ID, spec, createOptions) // è¿™é‡Œæœ€ç»ˆä¼šè°ƒç”¨ cotnainerd çš„ create
	// åˆ é™¤é”™è¯¯å¤„ç†ç›¸å…³ä»£ç 
	// TODO(mlaventure): we need to specify checkpoint options here
	pid, err := daemon.containerd.Start(context.Background(), container.ID, checkpointDir, // è¿™é‡Œéœ€è¦è·Ÿè¿›ä¸€ä¸‹
		container.StreamConfig.Stdin() != nil || container.Config.Tty,
		container.InitializeStdio)
  // å¿½ç•¥é”™è¯¯å¤„ç†
...
```

`containerStart` å‡½æ•°å‡†å¤‡ `containerd` è¿è¡Œçš„ä¸€åˆ‡éœ€è¦çš„ä¸œè¥¿åŒ…æ‹¬å­˜å‚¨å’Œç½‘ç»œã€‚ä½†æ˜¯å®é™…å¯åŠ¨å¯ä»¥å‘ç°æ˜¯è°ƒç”¨äº†`daemon.containerd.Create`ä¸`daemon.containerd.Start`æ¥å£ï¼Œç»†èŠ‚ä¸‹é¢è¿›ä¸€æ­¥åˆ†æã€‚

```go
func (c *client) Create(ctx context.Context, id string, ociSpec *specs.Spec, runtimeOptions interface{}) error {
	if ctr := c.getContainer(id); ctr != nil {
		return errors.WithStack(newConflictError("id already in use"))
	}

	bdir, err := prepareBundleDir(filepath.Join(c.stateDir, id), ociSpec) // å…¶å®å°±æ˜¯å‡†å¤‡å®¹å™¨å¯åŠ¨ç›®å½•ä¹‹ç±»çš„
	if err != nil {
		return errdefs.System(errors.Wrap(err, "prepare bundle dir failed"))
	}

	c.logger.WithField("bundle", bdir).WithField("root", ociSpec.Root.Path).Debug("bundle dir created")

	cdCtr, err := c.client.NewContainer(ctx, id, // æ ¸å¿ƒä»£ç ï¼Œæ›´å¤šçš„é€»è¾‘éœ€è¦ä¸‹é¢å±•å¼€
		containerd.WithSpec(ociSpec),
		// TODO(mlaventure): when containerd support lcow, revisit runtime value
		containerd.WithRuntime(fmt.Sprintf("io.containerd.runtime.v1.%s", runtime.GOOS), runtimeOptions))
	if err != nil {
		return wrapError(err)
	}

	c.Lock()
	c.containers[id] = &container{
		bundleDir: bdir,
		ctr:       cdCtr,
	}
	c.Unlock()

	return nil
}
```

`NewContainer()` å°±æ˜¯è¿™ä¸€æ®µä»£ç çš„æ ¸å¿ƒï¼Œéœ€è¦å±•å¼€è¯´æ˜

```go
// NewContainer will create a new container in container with the provided id
// the id must be unique within the namespace
func (c *Client) NewContainer(ctx context.Context, id string, opts ...NewContainerOpts) (Container, error) {
   ctx, done, err := c.WithLease(ctx)
   if err != nil {
      return nil, err
   }
   defer done(ctx)

   container := containers.Container{
      ID: id,
      Runtime: containers.RuntimeInfo{
         Name: c.runtime,
      },
   }
   for _, o := range opts {
      if err := o(ctx, c, &container); err != nil {
         return nil, err
      }
   }
   r, err := c.ContainerService().Create(ctx, container) // æ ¸å¿ƒå°±æ˜¯ create äº†
   if err != nil {
      return nil, err
   }
   return containerFromRecord(c, r), nil
}
```

å¯ä»¥çœ‹åˆ°`c.ContainerService().Create(ctx, container)`å…¶å®è°ƒç”¨äº† `container service`ï¼Œè¿™ä¸ªä¸ªæ˜¯ `docker` å±‚é¢çš„æŠ½è±¡ï¼Œå…·ä½“å®ç°æ˜¯ `containerd`æä¾›çš„ã€‚

```go
func (r *remoteContainers) Create(ctx context.Context, container containers.Container) (containers.Container, error) {
   created, err := r.client.Create(ctx, &containersapi.CreateContainerRequest{
      Container: containerToProto(&container),
...
```

```go
func (c *containersClient) Create(ctx context.Context, in *CreateContainerRequest, opts ...grpc.CallOption) (*CreateContainerResponse, error) {
	out := new(CreateContainerResponse)
	err := grpc.Invoke(ctx, "/containerd.services.containers.v1.Containers/Create", in, out, c.cc, opts...)
...
```

åŸºäºä¸Šè¿°`client.create`å…¶å®ä¹Ÿå¯ä»¥å‘ç°ï¼Œå®è´¨æ€§çš„å·¥ä½œåªæœ‰ä¸€ä¸ªå°±æ˜¯å‡†å¤‡å¯åŠ¨éœ€è¦ä¸€äº›ç›®å½•ç»“æ„ã€‚è¿™ä¸€åˆ‡éƒ½æ˜¯ä¸ºäº†å‡†å¤‡è°ƒç”¨ `/containerd.services.containers.v1.Containers/Create`æ¥å£ã€‚

`docker` è°ƒç”¨å®Œæˆ `containerd` çš„ `create` ä¸‹é¢å°±è¦è°ƒç”¨ `daemon.containerd.Start()`ã€‚

```go
// Start create and start a task for the specified containerd id
func (c *client) Start(ctx context.Context, id, checkpointDir string, withStdin bool, attachStdio StdioCallback) (int, error) {
	ctr := c.getContainer(id)
	if ctr == nil {
		return -1, errors.WithStack(newNotFoundError("no such container"))
	}
	if t := ctr.getTask(); t != nil {
		return -1, errors.WithStack(newConflictError("container already started"))
	}

	var (
		cp             *types.Descriptor
		t              containerd.Task
		rio            cio.IO
		err            error
		stdinCloseSync = make(chan struct{})
	)

	// å¿½ç•¥ checkpoint ç›¸å…³ä»£ç 

	spec, err := ctr.ctr.Spec(ctx)
	if err != nil {
		return -1, errors.Wrap(err, "failed to retrieve spec")
	}
	uid, gid := getSpecUser(spec)
	t, err = ctr.ctr.NewTask(ctx, // è¿™é‡Œå°±æ˜¯è°ƒç”¨ containerd, /containerd.services.tasks.v1.Tasks/Create
		func(id string) (cio.IO, error) {
			fifos := newFIFOSet(ctr.bundleDir, InitProcessName, withStdin, spec.Process.Terminal)

			rio, err = c.createIO(fifos, id, InitProcessName, stdinCloseSync, attachStdio)
			return rio, err
		},
		func(_ context.Context, _ *containerd.Client, info *containerd.TaskInfo) error {
			info.Checkpoint = cp
			info.Options = &runctypes.CreateOptions{
				IoUid:       uint32(uid),
				IoGid:       uint32(gid),
				NoPivotRoot: os.Getenv("DOCKER_RAMDISK") != "",
			}
			return nil
		})
// ç§»é™¤é”™è¯¯å¤„ç†ç›¸å…³ä»£ç 
	ctr.setTask(t)

	// Signal c.createIO that it can call CloseIO
	close(stdinCloseSync)

	if err := t.Start(ctx); err != nil { // è¿™ä¸ªtæ˜¯taskçš„ç¼©å†™, /containerd.services.tasks.v1.Tasks/Start
// ç§»é™¤é”™è¯¯å¤„ç†ç›¸å…³ä»£ç 
```

åŸºäºä¸Šè¿°ä»£ç å¯ä»¥çœ‹åˆ°ï¼Œè¿™é‡Œè¿˜æ˜¯äºŒæ¬¡å°è£…äº†ã€‚

`ctr.ctr.NewTask()` å°±æ˜¯å‡†å¤‡å¥½å‚æ•°å‡†å¤‡è°ƒç”¨ `containerd`çš„`/containerd.services.tasks.v1.Tasks/Create`æ¥å£ã€‚

```go
func (c *container) NewTask(ctx context.Context, ioCreate cio.Creator, opts ...NewTaskOpts) (_ Task, err error) {
  // åˆ é™¤ææ„å‡½æ•°å’Œé”™è¯¯å¤„ç†
	cfg := i.Config()
	request := &tasks.CreateTaskRequest{
		ContainerID: c.id,
		Terminal:    cfg.Terminal,
		Stdin:       cfg.Stdin,
		Stdout:      cfg.Stdout,
		Stderr:      cfg.Stderr,
	}
	// åˆ é™¤éƒ¨åˆ†ä»£ç 
	response, err := c.client.TaskService().Create(ctx, request) // task service create
	if err != nil {
		return nil, errdefs.FromGRPC(err)
	}
	t.pid = response.Pid
	return t, nil
}
```

```go
func (c *tasksClient) Create(ctx context.Context, in *CreateTaskRequest, opts ...grpc.CallOption) (*CreateTaskResponse, error) {
   out := new(CreateTaskResponse)
   err := grpc.Invoke(ctx, "/containerd.services.tasks.v1.Tasks/Create", in, out, c.cc, opts...)
   if err != nil {
      return nil, err
   }
   return out, nil
}
```

è€Œ`t.Start(ctx)`å‡½æ•°çš„å®ç°æ›´ç®€å•ï¼Œæœ€åè°ƒç”¨`containerd`çš„`/containerd.services.tasks.v1.Tasks/Start`æ¥å£

```go
func (c *tasksClient) Start(ctx context.Context, in *StartRequest, opts ...grpc.CallOption) (*StartResponse, error) {
	out := new(StartResponse)
	err := grpc.Invoke(ctx, "/containerd.services.tasks.v1.Tasks/Start", in, out, c.cc, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}
```

åŸºäºä»¥ä¸Šåˆ†æï¼Œå…¶å® `dockerd` çš„`ContainerStart`å…ˆåä¼šè°ƒç”¨åˆ°`cotnainerd`çš„`/containerd.services.containers.v1.Containers/Create`ï¼Œå…¶æ¬¡`/containerd.services.tasks.v1.Tasks/Create`ï¼Œæœ€åæ˜¯`/containerd.services.tasks.v1.Tasks/Start`ã€‚

é‚£ä¹ˆä¸‹é¢å°±å»èµ°è¯»ä¸€ä¸‹ `containerd`çš„ç›¸å…³å®ç°ã€‚

# containerd

æ ¹æ®çº¿ä¸Šä¿¡æ¯ `checkout `å‡º`containerd`ç‰ˆæœ¬ä¸º`7ad184331fa3e55e52b890ea95e65ba581ae3429`çš„ä»£ç è¿›è¡Œèµ°è¯»ã€‚æ ¹æ®ä¸Šä¸€ç¯‡ç»“æŸçš„æ¥å£ä¿¡æ¯ï¼Œç›´æ¥æœç´¢ç›¸å…³`GRPC`æ¥å£å¯ä»¥æ‰¾åˆ°å®ç°çš„ `handler`ã€‚

```go
func _Containers_Create_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor 
...
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/containerd.services.containers.v1.Containers/Create",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(ContainersServer).Create(ctx, req.(*CreateContainerRequest))
...    
```

```go
func _Tasks_Create_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
...
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/containerd.services.tasks.v1.Tasks/Create",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(TasksServer).Create(ctx, req.(*CreateTaskRequest))
...
```

```go
func _Tasks_Start_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
...
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/containerd.services.tasks.v1.Tasks/Start",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(TasksServer).Start(ctx, req.(*StartRequest))
...
```

é‚£ä¹ˆè¿™ä¸€ç¯‡åˆ†åˆ«èµ°è¯»ä¸€ä¸‹ä¸‰ä¸ªæ¥å£æ˜¯å¦‚ä½•å®ç°çš„ã€‚

## ContainersServer.Create æ–¹æ³•çš„å®ç°

å†æ¥çœ‹ä¸€ä¸‹`create`å®ç°ï¼Œé€šè¿‡ä¹‹å‰ api é…åˆ IDE å°±èƒ½ç›´æ¥è½¬è·³åˆ°ä¸‹é¢çš„ä»£ç å®ç°ã€‚

```go
func (l *local) Create(ctx context.Context, req *api.CreateContainerRequest, _ ...grpc.CallOption) (*api.CreateContainerResponse, error) {
	var resp api.CreateContainerResponse

	if err := l.withStoreUpdate(ctx, func(ctx context.Context, store containers.Store) error {
		container := containerFromProto(&req.Container)

		created, err := store.Create(ctx, container) // å°†containerçš„ä¿¡æ¯å†™å…¥æœ¬åœ°çš„ä¸€ä¸ªæ•°æ®åº“
...
```
é€šè¿‡å¯¹ä¸Šé¢ä»£ç èµ°è¯»åˆ†æï¼Œå‘ç°å…¶å® `cotnainerd` çš„ `create` æ“ä½œä»…ä»…å°±æ˜¯å†™ä¸€ä¸ªæ•°æ®åˆ° `bucket ` ä¸­å»ï¼Œè¿˜æœ‰ä¸€ä¸ª `event`ä¿¡æ¯ `create container`ã€‚ç›®å‰è¿˜ä¸æ˜ç™½è¿™ä¸ª `event` é™¤äº†ä½¿ç”¨ `ctr events`çœ‹åˆ°è¿˜æœ‰å…¶ä»–çš„ä½œç”¨ä¹ˆï¼Ÿ

## TasksServer.Create æ–¹æ³•çš„å®ç°

`containerd` ä½¿ç”¨ `task` æ¥ç®¡ç† `container` çš„åˆ›å»ºå’Œåˆ é™¤ï¼Œåœ¨ `containerd` çš„ `readme` æ–‡æ¡£ä¸­ä¹Ÿå†™çš„æ¯”è¾ƒæ¸…æ¥šäº†ã€‚

```go
func (l *local) Create(ctx context.Context, r *api.CreateTaskRequest, _ ...grpc.CallOption) (*api.CreateTaskResponse, error) {
	var (
		checkpointPath string
		err            error
	)

	container, err := l.getContainer(ctx, r.ContainerID) // å‰é¢å†™ dbï¼Œè¿™é‡Œè¯» db è·å– cotnainer çš„ä¿¡æ¯ã€‚
	if err != nil {
		return nil, errdefs.ToGRPC(err)
	}
	opts := runtime.CreateOpts{
		Spec: container.Spec,
		IO: runtime.IO{
			Stdin:    r.Stdin,
			Stdout:   r.Stdout,
			Stderr:   r.Stderr,
			Terminal: r.Terminal,
		},
		Checkpoint:     checkpointPath,
		Runtime:        container.Runtime.Name,
		RuntimeOptions: container.Runtime.Options,
		TaskOptions:    r.Options,
	}
	for _, m := range r.Rootfs {
		opts.Rootfs = append(opts.Rootfs, mount.Mount{
			Type:    m.Type,
			Source:  m.Source,
			Options: m.Options,
		})
	}
	runtime, err := l.getRuntime(container.Runtime.Name) // runtime åå­—ï¼Œç°åœ¨æœ‰ v1.linux, v2 ä¸¤ä¸ªå®ç°
	if err != nil {
		return nil, err
	}
	c, err := runtime.Create(ctx, r.ContainerID, opts) // ç›®å‰çº¿ä¸Šä½¿ç”¨ v1 ï¼ˆç›®å‰æ˜¯é€šè¿‡ containerd-shim åé¢çš„å‚æ•°åˆ¤æ–­å‡ºæ¥çš„ï¼Œv1/v2 å‚æ•°å·®åˆ«å¾ˆæ˜æ˜¾
	if err != nil {
		return nil, errdefs.ToGRPC(err)
	}
	// TODO: fast path for getting pid on create
	if err := l.monitor.Monitor(c); err != nil {
		return nil, errors.Wrap(err, "monitor task")
	}
	state, err := c.State(ctx)
	if err != nil {
		log.G(ctx).Error(err)
	}
	return &api.CreateTaskResponse{
		ContainerID: r.ContainerID,
		Pid:         state.Pid,
	}, nil
}
```

è¿™é‡Œä½¿ç”¨ `v1 Linux runtime` çš„`create` æ–¹æ³•

```go
// Create a new task
func (r *Runtime) Create(ctx context.Context, id string, opts runtime.CreateOpts) (_ runtime.Task, err error) {
	namespace, err := namespaces.NamespaceRequired(ctx) // è¿™é‡Œ ns æ˜¯ mobyï¼Œå°±æ˜¯ docker é‚£ä¸ªæ–°çš„åå­—
	if err != nil {
		return nil, err
	}

	if err := identifiers.Validate(id); err != nil {
		return nil, errors.Wrapf(err, "invalid task id")
	}

	ropts, err := r.getRuncOptions(ctx, id)
	if err != nil {
		return nil, err
	}

	bundle, err := newBundle(id, // newBundle creates a new bundle on disk at the provided path for the given id
		filepath.Join(r.state, namespace),
		filepath.Join(r.root, namespace),
		opts.Spec.Value)
	if err != nil {
		return nil, err
	}
	defer func() {
		if err != nil {
			bundle.Delete()
		}
	}()

  // ShimLocal is a ShimOpt for using an in process shim implementation
	shimopt := ShimLocal(r.config, r.events)
	if !r.config.NoShim { // åœ¨æ­£å¸¸çš„é€»è¾‘ä¼šå‘½ä¸­è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨äº† shim
		var cgroup string
		if opts.TaskOptions != nil {
			v, err := typeurl.UnmarshalAny(opts.TaskOptions)
			if err != nil {
				return nil, err
			}
			cgroup = v.(*runctypes.CreateOptions).ShimCgroup
		}
		exitHandler := func() {
			log.G(ctx).WithField("id", id).Info("shim reaped")
			t, err := r.tasks.Get(ctx, id)
			if err != nil {
				// Task was never started or was already successfully deleted
				return
			}
			lc := t.(*Task)

			log.G(ctx).WithFields(logrus.Fields{
				"id":        id,
				"namespace": namespace,
			}).Warn("cleaning up after killed shim")
			if err = r.cleanupAfterDeadShim(context.Background(), bundle, namespace, id, lc.pid); err != nil {
				log.G(ctx).WithError(err).WithFields(logrus.Fields{
					"id":        id,
					"namespace": namespace,
				}).Warn("failed to clean up after killed shim")
			}
		}   
		shimopt = ShimRemote(r.config, r.address, cgroup, exitHandler) 
    // è¿™é‡Œç›®å‰åªæ˜¯æ„å»ºå¥½äº† shimopt è¿™ä¸ªå‡½æ•°ï¼Œç›®å‰è¿˜æ²¡æœ‰çœŸçš„è°ƒç”¨
	}

	// shimopt è¿™ä¸ªå‡½æ•°å‡†å¤‡å¥½äº†ï¼Œè¿™é‡Œæ‰å¼€å§‹è°ƒç”¨å¯åŠ¨ shim å¹¶è¿”å› shim client
	s, err := bundle.NewShimClient(ctx, namespace, shimopt, ropts) 
	if err != nil {
		return nil, err
	}
	defer func() {
		if err != nil {
			if kerr := s.KillShim(ctx); kerr != nil {
				log.G(ctx).WithError(err).Error("failed to kill shim")
			}
		}
	}()

	rt := r.config.Runtime
	if ropts != nil && ropts.Runtime != "" {
		rt = ropts.Runtime
	}
	sopts := &shim.CreateTaskRequest{
		ID:         id,
		Bundle:     bundle.path,
		Runtime:    rt,
		Stdin:      opts.IO.Stdin,
		Stdout:     opts.IO.Stdout,
		Stderr:     opts.IO.Stderr,
		Terminal:   opts.IO.Terminal,
		Checkpoint: opts.Checkpoint,
		Options:    opts.TaskOptions,
	}
	for _, m := range opts.Rootfs {
		sopts.Rootfs = append(sopts.Rootfs, &types.Mount{
			Type:    m.Type,
			Source:  m.Source,
			Options: m.Options,
		})
	}
	// Create a new initial process and container with the underlying OCI runtime 
  // 
	cr, err := s.Create(ctx, sopts) // ä¹Ÿå°±æ˜¯ shim çš„ create æ–¹æ³•ï¼ŒæŒ‰ä½ä¸è¡¨åˆ°è®² containerd-shim å†è¯´ã€‚
	if err != nil {
		return nil, errdefs.FromGRPC(err)
	}
	t, err := newTask(id, namespace, int(cr.Pid), s, r.events, r.tasks, bundle)
	if err != nil {
		return nil, err
	}
	if err := r.tasks.Add(ctx, t); err != nil {
		return nil, err
	}
	// ... ç§»é™¤ä¸€éƒ¨åˆ† event ç›¸å…³é€»è¾‘
	return t, nil
}
```



åŸºäºä»¥ä¸Šå…¶å®å°±å·²ç»å‡†å¤‡å¥½äº† `task` å¹¶å¯åŠ¨ ï¼Œåœ¨ `cotnainerd` ä¸­è°ƒç”¨ä½¿ç”¨å‘½ä»¤è¡Œè°ƒç”¨ `containerd-shim`ã€‚å…¶ä½™çš„é€»è¾‘è¦åˆ° `cotnainerd-shim` ä¸­å»æ¢ç´¢äº†ã€‚

##  TasksServer.Start æ–¹æ³•çš„å®ç°

`create` æ“ä½œçœ‹å®Œäº†çœ‹ä¸€ä¸‹ `start` æ–¹æ³•æ˜¯å¦‚ä½•å®ç°çš„ï¼Œæœç´¢å…¶å¯¹åº”çš„æ¥å£ `/containerd.services.tasks.v1.Tasks/Start`


```go
func (s *service) Start(ctx context.Context, r *api.StartRequest) (*api.StartResponse, error) {
	return s.local.Start(ctx, r)
```

ä¾ç„¶æ˜¯ `v1.linux` å®ç°

```go
func (l *local) Start(ctx context.Context, r *api.StartRequest, _ ...grpc.CallOption) (*api.StartResponse, error) {
	t, err := l.getTask(ctx, r.ContainerID) // è¿™é‡Œå°±æ˜¯ä» bucket db ä¸­è·å–æ•°æ®
	if err != nil {
		return nil, err
	}
	p := runtime.Process(t)
	if r.ExecID != "" {
		if p, err = t.Process(ctx, r.ExecID); err != nil {
			return nil, errdefs.ToGRPC(err)
		}
	}
	if err := p.Start(ctx); err != nil { // Start the container's user defined process
		return nil, errdefs.ToGRPC(err)
	}
	state, err := p.State(ctx)
	if err != nil {
		return nil, errdefs.ToGRPC(err) // ToGRPC
	}
	return &api.StartResponse{
		Pid: state.Pid,
	}, nil
...
```

```go
// Start the task
func (t *Task) Start(ctx context.Context) error {
	t.mu.Lock()
	hasCgroup := t.cg != nil
	t.mu.Unlock()
	r, err := t.shim.Start(ctx, &shim.StartRequest{ // è°ƒç”¨ shim çš„ start
		ID: t.id,
...
```

åŸºäºä»¥ä¸Šä¸¤ä¸ªæ¥å£å…¶å®å‘ç°æ›´å¤šå·¥ä½œ `containerd` æ”¾åˆ°äº† `conteinrd-shim` ä¸­å®Œæˆäº†ã€‚

# containerd-shim çš„å¦‚ä½•å·¥ä½œ

åˆçœ‹ `containerd-shim` çš„å®ç°æ¨¡å‹ä¹Ÿä¸æ˜¯å¾ˆå¤æ‚ï¼Œä¸»å‡½æ•°ä¸­è°ƒç”¨ä¸€ä¸ª `rpc` æœåŠ¡å¸¸é©»åå°ï¼Œå¯¹å¤–æä¾›æœåŠ¡ã€‚

```go
func main() {
...

	if err := executeShim(); err != nil {
```

```go
func executeShim() error {
....
	}
	server, err := newServer()
	if err != nil {
		return errors.Wrap(err, "failed creating server")
	}
	sv, err := shim.NewService(
		shim.Config{
			Path:          path,
			Namespace:     namespaceFlag,
			WorkDir:       workdirFlag,
			Criu:          criuFlag,
			SystemdCgroup: systemdCgroupFlag,
			RuntimeRoot:   runtimeRootFlag,
		},
		&remoteEventsPublisher{address: addressFlag},
	)
	if err != nil {
		return err
	}
	logrus.Debug("registering ttrpc server")
	shimapi.RegisterShimService(server, sv) // ttrpc 

	socket := socketFlag
	if err := serve(context.Background(), server, socket); err != nil {
		return err
	}
....
```

å”¯ä¸€å€¼å¾—ä¸€æçš„å°±æ˜¯`ttrpc`æ˜¯ä½å†…å­˜ä¸‹é¢çš„ rpc åè®®ï¼ŒåŸºäº grpc çš„ä½å†…å­˜ç‰ˆæœ¬ã€‚

```go
func RegisterShimService(srv *ttrpc.Server, svc ShimService) {
	srv.Register("containerd.runtime.linux.shim.v1.Shim", map[string]ttrpc.Method{
...
		"Create": func(ctx context.Context, unmarshal func(interface{}) error) (interface{}, error) {
			var req CreateTaskRequest
			if err := unmarshal(&req); err != nil {
				return nil, err
			}
			return svc.Create(ctx, &req)
		},
		"Start": func(ctx context.Context, unmarshal func(interface{}) error) (interface{}, error) {
			var req StartRequest
			if err := unmarshal(&req); err != nil {
				return nil, err
			}
			return svc.Start(ctx, &req)
		},
```

è¿™æ¬¡åªå…³å¿ƒ `shim` çš„ `Create` ä¸ `Start` å®ç°ã€‚

## contained-shim create

è¿™ä¸ªå°±æ˜¯`containerd`çš„`s.Create(ctx, sopts)`å®ç°çš„åœ°æ–¹

```go
func (c *local) Create(ctx context.Context, in *shimapi.CreateTaskRequest) (*shimapi.CreateTaskResponse, error) {
	return c.s.Create(ctx, in)
...
```

`create`

```go
// Create a new initial process and container with the underlying OCI runtime
func (s *Service) Create(ctx context.Context, r *shimapi.CreateTaskRequest) (_ *shimapi.CreateTaskResponse, err error) {
	var mounts []proc.Mount
	for _, m := range r.Rootfs {
		mounts = append(mounts, proc.Mount{
			Type:    m.Type,
			Source:  m.Source,
			Target:  m.Target,
			Options: m.Options,
		})
	}

	config := &proc.CreateConfig{
		ID:               r.ID,
		Bundle:           r.Bundle,
		Runtime:          r.Runtime,
		Rootfs:           mounts,
		Terminal:         r.Terminal,
		Stdin:            r.Stdin,
		Stdout:           r.Stdout,
		Stderr:           r.Stderr,
		Checkpoint:       r.Checkpoint,
		ParentCheckpoint: r.ParentCheckpoint,
		Options:          r.Options,
	}
	rootfs := filepath.Join(r.Bundle, "rootfs")
	defer func(rootfs string) {
		if err != nil {
			if err2 := mount.UnmountAll(rootfs, 0); err2 != nil {
				log.G(ctx).WithError(err2).Warn("Failed to cleanup rootfs mount")
			}
		}
	}(rootfs)
	for _, rm := range mounts {
		m := &mount.Mount{
			Type:    rm.Type,
			Source:  rm.Source,
			Options: rm.Options,
		}
		if err := m.Mount(rootfs); err != nil {
			return nil, errors.Wrapf(err, "failed to mount rootfs component %v", m)
		}
	}

	s.mu.Lock()
	defer s.mu.Unlock()

	if len(mounts) == 0 {
		rootfs = ""
	}

	process, err := newInit(
		ctx,
		s.config.Path,
		s.config.WorkDir,
		s.config.RuntimeRoot,
		s.config.Namespace,
		s.config.Criu,
		s.config.SystemdCgroup,
		s.platform,
		config,
		rootfs,
	)
	if err != nil {
		return nil, errdefs.ToGRPC(err)
	}
	if err := process.Create(ctx, config); err != nil {
		return nil, errdefs.ToGRPC(err)
	}
	// save the main task id and bundle to the shim for additional requests
	s.id = r.ID
	s.bundle = r.Bundle
	pid := process.Pid()
	s.processes[r.ID] = process
	return &shimapi.CreateTaskResponse{
		Pid: uint32(pid),
	}, nil
}

```

```go
// Create the process with the provided config
func (p *Init) Create(ctx context.Context, r *CreateConfig) error {
	var (
		err    error
		socket *runc.Socket
	)

	if r.Terminal {
		if socket, err = runc.NewTempConsoleSocket(); err != nil {
			return errors.Wrap(err, "failed to create OCI runtime console socket")
		}
		defer socket.Close()
	} else if hasNoIO(r) {
		if p.io, err = runc.NewNullIO(); err != nil {
			return errors.Wrap(err, "creating new NULL IO")
		}
	} else {
		if p.io, err = runc.NewPipeIO(p.IoUID, p.IoGID, withConditionalIO(p.stdio)); err != nil {
			return errors.Wrap(err, "failed to create OCI runtime io pipes")
		}
	}
	pidFile := filepath.Join(p.Bundle, InitPidFile)
	if r.Checkpoint != "" {
		opts := &runc.RestoreOpts{
			CheckpointOpts: runc.CheckpointOpts{
				ImagePath:  r.Checkpoint,
				WorkDir:    p.WorkDir,
				ParentPath: r.ParentCheckpoint,
			},
			PidFile:     pidFile,
			IO:          p.io,
			NoPivot:     p.NoPivotRoot,
			Detach:      true,
			NoSubreaper: true,
		}
		p.initState = &createdCheckpointState{
			p:    p,
			opts: opts,
		}
		return nil
	}
	opts := &runc.CreateOpts{
		PidFile:      pidFile,
		IO:           p.io,
		NoPivot:      p.NoPivotRoot,
		NoNewKeyring: p.NoNewKeyring,
	}
	if socket != nil {
		opts.ConsoleSocket = socket
	}
	if err := p.runtime.Create(ctx, r.ID, r.Bundle, opts); err != nil {
		return p.runtimeError(err, "OCI runtime create failed")
	}
	if r.Stdin != "" {
		sc, err := fifo.OpenFifo(context.Background(), r.Stdin, syscall.O_WRONLY|syscall.O_NONBLOCK, 0)
		if err != nil {
			return errors.Wrapf(err, "failed to open stdin fifo %s", r.Stdin)
		}
		p.stdin = sc
		p.closers = append(p.closers, sc)
	}
	var copyWaitGroup sync.WaitGroup
	ctx, cancel := context.WithTimeout(ctx, 30*time.Second)
	defer cancel()
	if socket != nil {
		console, err := socket.ReceiveMaster()
		if err != nil {
			return errors.Wrap(err, "failed to retrieve console master")
		}
		console, err = p.Platform.CopyConsole(ctx, console, r.Stdin, r.Stdout, r.Stderr, &p.wg, &copyWaitGroup)
		if err != nil {
			return errors.Wrap(err, "failed to start console copy")
		}
		p.console = console
	} else if !hasNoIO(r) {
		if err := copyPipes(ctx, p.io, r.Stdin, r.Stdout, r.Stderr, &p.wg, &copyWaitGroup); err != nil {
			return errors.Wrap(err, "failed to start io pipe copy")
		}
	}

	copyWaitGroup.Wait()
	pid, err := runc.ReadPidFile(pidFile)
	if err != nil {
		return errors.Wrap(err, "failed to retrieve OCI runtime container pid")
	}
	p.pid = pid
	return nil
}
```

```go
// Create creates a new container and returns its pid if it was created successfully
func (r *Runc) Create(context context.Context, id, bundle string, opts *CreateOpts) error {
	args := []string{"create", "--bundle", bundle} // è¿™é‡Œå°±æ˜¯ runc create --bundle çš„å‘½ä»¤è¡Œäº†
	if opts != nil {
		oargs, err := opts.args()
		if err != nil {
			return err
		}
		args = append(args, oargs...)
	}
	cmd := r.command(context, append(args, id)...)
	if opts != nil && opts.IO != nil {
		opts.Set(cmd)
	}
	cmd.ExtraFiles = opts.ExtraFiles

	if cmd.Stdout == nil && cmd.Stderr == nil {
		data, err := cmdOutput(cmd, true)
		if err != nil {
			return fmt.Errorf("%s: %s", err, data)
		}
		return nil
	}
...
```

## contained-shim start

åŸºäº containerd çš„åˆ†æå…¶å®è¿˜çœ‹åˆ°è°ƒç”¨ `t.shim.Start(ctx, &shim.StartRequest{ID: t.id})`

```go
func (c *local) Start(ctx context.Context, in *shimapi.StartRequest) (*shimapi.StartResponse, error) {
   return c.s.Start(ctx, in)  
```

```go
// Start a process
func (s *Service) Start(ctx context.Context, r *shimapi.StartRequest) (*shimapi.StartResponse, error) {
   p, err := s.getExecProcess(r.ID) // get exec process 
   if err != nil {
      return nil, err
   }
   if err := p.Start(ctx); err != nil {
      return nil, err
   }
   return &shimapi.StartResponse{
      ID:  p.ID(),
      Pid: uint32(p.Pid()),
   }, nil
```

```go
func (e *execProcess) Start(ctx context.Context) error {
   e.mu.Lock()
   defer e.mu.Unlock()

   return e.execState.Start(ctx) // execState è¿™ä¼šåº”è¯¥æ˜¯ createdï¼Œå› ä¸ºå‰é¢å·²ç» runc create --bundle äº†
```

```go
func (s *createdState) Start(ctx context.Context) error {
	if err := s.p.start(ctx); err != nil {
```

```go
func (p *Init) start(ctx context.Context) error {
	err := p.runtime.Start(ctx, p.id)
```

```go
// Start will start an already created container
func (r *Runc) Start(context context.Context, id string) error {
   return r.runOrError(r.command(context, "start", id)) // è¿™é‡Œä¼šè¿è¡Œï¼Œr.command è¿”å›çš„ exec.Cmd object 
}
```

```go
func (r *Runc) command(context context.Context, args ...string) *exec.Cmd {
   command := r.Command
   if command == "" {
      command = DefaultCommand // DefaultCommand = "runc"
   }
   cmd := exec.CommandContext(context, command, append(r.args(), args...)...)
   cmd.SysProcAttr = &syscall.SysProcAttr{
      Setpgid: r.Setpgid,
   }
   cmd.Env = filterEnv(os.Environ(), "NOTIFY_SOCKET") // NOTIFY_SOCKET introduces a special behavior in runc but should only be set if invoked from systemd
   if r.PdeathSignal != 0 {
      cmd.SysProcAttr.Pdeathsig = r.PdeathSignal
   }

   return cmd
}
```



é‚£ä¹ˆå…¶å®åˆ°è¿™é‡Œä¹Ÿå°±çœ‹å®Œäº† `containrd-shim` è°ƒç”¨ `runc create `/ `runc start `

# runc 

å‰é¢çœ‹è¿‡äº† `containerd-shim` è°ƒç”¨äº† `runc create` ä¸ `runc start`ï¼Œè¿™é‡Œæ¢³ç†ä¸€ä¸‹ `runc` çš„ç›¸å…³ä»£ç é€»è¾‘ã€‚

## run create --bundle

```go
func startContainer(context *cli.Context, spec *specs.Spec, action CtAct, criuOpts *libcontainer.CriuOpts) (int, error) {
	id := context.Args().First()
	if id == "" {
		return -1, errEmptyID
	}

	notifySocket := newNotifySocket(context, os.Getenv("NOTIFY_SOCKET"), id)
	if notifySocket != nil {
		notifySocket.setupSpec(context, spec)
	}

	container, err := createContainer(context, id, spec) // å‡†å¤‡ container åœ¨ runc å†…å­˜ä¸­çš„ object
	if err != nil {
		return -1, err
	}

	if notifySocket != nil {
		err := notifySocket.setupSocket()
		if err != nil {
			return -1, err
		}
	}

	// Support on-demand socket activation by passing file descriptors into the container init process.
	listenFDs := []*os.File{}
	if os.Getenv("LISTEN_FDS") != "" {
		listenFDs = activation.Files(false)
	}

	logLevel := "info"
	if context.GlobalBool("debug") {
		logLevel = "debug"
	}

	r := &runner{
		enableSubreaper: !context.Bool("no-subreaper"),
		shouldDestroy:   true,
		container:       container,
		listenFDs:       listenFDs,
		notifySocket:    notifySocket,
		consoleSocket:   context.String("console-socket"),
		detach:          context.Bool("detach"),
		pidFile:         context.String("pid-file"),
		preserveFDs:     context.Int("preserve-fds"),
		action:          action,
		criuOpts:        criuOpts,
		init:            true,
		logLevel:        logLevel,
	}
	return r.run(spec.Process) // run
}
```

è°ƒç”¨ `createContainer`åˆ›å»º `cotnainer` å¯¹è±¡ï¼Œå®é™…çš„åˆ›å»ºç”±å¹³å°ç›¸å…³çš„å·¥å‚å‡½æ•°å®ç°

```go
func createContainer(context *cli.Context, id string, spec *specs.Spec) (libcontainer.Container, error) {
	rootlessCg, err := shouldUseRootlessCgroupManager(context)
	if err != nil {
		return nil, err
	}
	config, err := specconv.CreateLibcontainerConfig(&specconv.CreateOpts{
		CgroupName:       id,
		UseSystemdCgroup: context.GlobalBool("systemd-cgroup"),
		NoPivotRoot:      context.Bool("no-pivot"),
		NoNewKeyring:     context.Bool("no-new-keyring"),
		Spec:             spec,
		RootlessEUID:     os.Geteuid() != 0,
		RootlessCgroups:  rootlessCg,
	})
	if err != nil {
		return nil, err
	}

	factory, err := loadFactory(context) // è¿”å› Linux çš„å®ç° InitPath å­—æ®µä¸º /proc/self/exe ï¼ŒInitArgs å­—æ®µä¸º []string{os.Args[0], "init"},
	if err != nil {
		return nil, err
	}
	return factory.Create(id, config)
```

`factory.Create` åˆ›å»ºè¿”å›ä¸€ä¸ª `container`ç»“æ„ä½“ï¼Œå…¶ä¸­ container ç»“æ„ä½“

```go
	c := &linuxContainer{
		id:            id,
		root:          containerRoot, //è¿™é‡Œæ˜¯ cotnainer çš„ root ç›®å½•
		config:        config, 
		initPath:      l.InitPath, // init path /proc/self/exe
		initArgs:      l.InitArgs, // []string{os.Args[0].}
		criuPath:      l.CriuPath,
		newuidmapPath: l.NewuidmapPath,
		newgidmapPath: l.NewgidmapPath,
		cgroupManager: l.NewCgroupsManager(config.Cgroups, nil),
	}
```

è¿”å›ä¸Šè¿°ç»“æ„ä½“ï¼Œç»§ç»­å›åˆ°åˆ°ä¸Šå±‚ä»£ç ç»§ç»­æ‰§è¡Œåˆ°`r.run(spec.Process)`

```go
func (r *runner) run(config *specs.Process) (int, error) {
...
	process, err := newProcess(*config, r.init, r.logLevel)
	if err != nil {
		return -1, err
	}
....

	switch r.action {
	case CT_ACT_CREATE:
		err = r.container.Start(process) // è¿™æ¬¡è¡Œä¸ºä¼šåˆ°è¿™ä¸ªæµç¨‹
....
```

å›åˆ°` r.container.Start(process)` ç»§ç»­æ‰§è¡Œ

```go
func (c *linuxContainer) Start(process *Process) error {
	c.m.Lock()
	defer c.m.Unlock()
	if process.Init { // ä½¿ç”¨ create è¿™é‡Œå°±æ˜¯ true
		if err := c.createExecFifo(); err != nil { // create ä¸€ä¸ª exec.fifo ç”¨ä¸è¿›ç¨‹é—´é€šä¿¡, åªæœ‰å†™æ—¶ä¼šè¢«é˜»å¡ï¼Œè¯»å†™éƒ½åœ¨æ—¶æ‰ä¼šæ­£å¸¸è¿è¡Œ
			return err
		}
	}
	if err := c.start(process); err != nil { // è¿™æ‰æ˜¯æœ¬æ¬¡å…³æ³¨ç‚¹
		if process.Init {
			c.deleteExecFifo()
		}
		return err
	}
	return nil
}
```

`c.start(process)`çš„å®ç°

```go
func (c *linuxContainer) start(process *Process) error {
	parent, err := c.newParentProcess(process) // åˆ›å»ºçˆ¶è¿›ç¨‹ï¼Œ ä»£ç åœ¨ä¸‹é¢ review
	if err != nil {
		return newSystemErrorWithCause(err, "creating new parent process")
	}
	parent.forwardChildLogs()
	if err := parent.start(); err != nil { // è¿™é‡Œåˆ›å»ºçˆ¶è¿›ç¨‹çš„ startï¼Œå…¶å®ä¹Ÿå°±æ˜¯ runc init
		// terminate the process to ensure that it properly is reaped.
...
```

`newParentProcess()`å…¶å®å°±æ˜¯å‘½ä»¤è¡Œä¸º` runc init` çš„ `parentProcess`ï¼Œè¿”å›ç»™ä¸Šé¢è°ƒç”¨`parent.start()`ã€‚

```go
func (c *linuxContainer) newParentProcess(p *Process) (parentProcess, error) {
	parentInitPipe, childInitPipe, err := utils.NewSockPair("init")
	if err != nil {
		return nil, newSystemErrorWithCause(err, "creating new init pipe")
	}
	messageSockPair := filePair{parentInitPipe, childInitPipe}

	parentLogPipe, childLogPipe, err := os.Pipe()
	if err != nil {
		return nil, fmt.Errorf("Unable to create the log pipe:  %s", err)
	}
	logFilePair := filePair{parentLogPipe, childLogPipe}

	cmd, err := c.commandTemplate(p, childInitPipe, childLogPipe) // å‡†å¤‡å‘½ä»¤è¡Œ 
	if err != nil {
		return nil, newSystemErrorWithCause(err, "creating new command template")
	}
	if !p.Init {
		return c.newSetnsProcess(p, cmd, messageSockPair, logFilePair) // å¦‚æœä¸æ˜¯ init ï¼Œæ‰€ä»¥è¿™ä¸€æ¬¡ä¸å…³å¿ƒè¿™é‡Œ
	}

	// We only set up fifoFd if we're not doing a `runc exec`. The historic
	// reason for this is that previously we would pass a dirfd that allowed
	// for container rootfs escape (and not doing it in `runc exec` avoided
	// that problem), but we no longer do that. However, there's no need to do
	// this for `runc exec` so we just keep it this way to be safe.
	if err := c.includeExecFifo(cmd); err != nil {
		return nil, newSystemErrorWithCause(err, "including execfifo in cmd.Exec setup")
	}
	return c.newInitProcess(p, cmd, messageSockPair, logFilePair) // è¿”å› initProcessï¼Œå…¶ä¸­ cmd ä¸º runc initï¼Œå¹¶ c.initProcess = init
}
```

`parent.start()`  å°±æ˜¯å®é™…å¼€å§‹è¿è¡Œ `runc init`äº†

````go
func (p *initProcess) start() error {
	defer p.messageSockPair.parent.Close()
	err := p.cmd.Start() // å¯åŠ¨ runc init
	p.process.ops = p
	// close the write-side of the pipes (controlled by child)
	p.messageSockPair.child.Close()
	p.logFilePair.child.Close()
	if err != nil {
		p.process.ops = nil
		return newSystemErrorWithCause(err, "starting init process command")
	}
	// Do this before syncing with child so that no children can escape the
	// cgroup. We don't need to worry about not doing this and not being root
	// because we'd be using the rootless cgroup manager in that case.
	if err := p.manager.Apply(p.pid()); err != nil {
		return newSystemErrorWithCause(err, "applying cgroup configuration for process")
	}
	if p.intelRdtManager != nil {
		if err := p.intelRdtManager.Apply(p.pid()); err != nil {
			return newSystemErrorWithCause(err, "applying Intel RDT configuration for process")
		}
	}
	defer func() {
		if err != nil {
			// TODO: should not be the responsibility to call here
			p.manager.Destroy()
			if p.intelRdtManager != nil {
				p.intelRdtManager.Destroy()
			}
		}
	}()

	if _, err := io.Copy(p.messageSockPair.parent, p.bootstrapData); err != nil {
		return newSystemErrorWithCause(err, "copying bootstrap data to pipe")
	}
	childPid, err := p.getChildPid() // è·å– child çš„ pid
	if err != nil {
		return newSystemErrorWithCause(err, "getting the final child's pid from pipe")
	}

	// Save the standard descriptor names before the container process
	// can potentially move them (e.g., via dup2()).  If we don't do this now,
	// we won't know at checkpoint time which file descriptor to look up.
	fds, err := getPipeFds(childPid)
	if err != nil {
		return newSystemErrorWithCausef(err, "getting pipe fds for pid %d", childPid)
	}
	p.setExternalDescriptors(fds)
	// Do this before syncing with child so that no children
	// can escape the cgroup
	if err := p.manager.Apply(childPid); err != nil { // è¿™é‡Œè®¾ç½®è°ƒç”¨å…·ä½“çš„å®ç°é…ç½® cgroup
		return newSystemErrorWithCause(err, "applying cgroup configuration for process")
	}
	if p.intelRdtManager != nil {
		if err := p.intelRdtManager.Apply(childPid); err != nil {
			return newSystemErrorWithCause(err, "applying Intel RDT configuration for process")
		}
	}
	// Now it's time to setup cgroup namesapce
	if p.config.Config.Namespaces.Contains(configs.NEWCGROUP) && p.config.Config.Namespaces.PathOf(configs.NEWCGROUP) == "" {
		if _, err := p.messageSockPair.parent.Write([]byte{createCgroupns}); err != nil {
			return newSystemErrorWithCause(err, "sending synchronization value to init process")
		}
	}

	// Wait for our first child to exit
	if err := p.waitForChildExit(childPid); err != nil {
		return newSystemErrorWithCause(err, "waiting for our first child to exit")
	}

	defer func() {
		if err != nil {
			// TODO: should not be the responsibility to call here
			p.manager.Destroy()
			if p.intelRdtManager != nil {
				p.intelRdtManager.Destroy()
			}
		}
	}()
	if err := p.createNetworkInterfaces(); err != nil { // åˆ›å»ºç½‘ç»œæ¥å£
		return newSystemErrorWithCause(err, "creating network interfaces")
	}
	if err := p.sendConfig(); err != nil { // æŠŠé…ç½®å‘é€ç»™å­è¿›ç¨‹
		return newSystemErrorWithCause(err, "sending config to init process")
	}
	var (
		sentRun    bool
		sentResume bool
	)

	ierr := parseSync(p.messageSockPair.parent, func(sync *syncT) error {
		switch sync.Type {
		case procReady:
			// set rlimits, this has to be done here because we lose permissions
			// to raise the limits once we enter a user-namespace
			if err := setupRlimits(p.config.Rlimits, p.pid()); err != nil {
				return newSystemErrorWithCause(err, "setting rlimits for ready process")
			}
			// call prestart hooks
			if !p.config.Config.Namespaces.Contains(configs.NEWNS) {
				// Setup cgroup before prestart hook, so that the prestart hook could apply cgroup permissions.
				if err := p.manager.Set(p.config.Config); err != nil {
					return newSystemErrorWithCause(err, "setting cgroup config for ready process")
				}
				if p.intelRdtManager != nil {
					if err := p.intelRdtManager.Set(p.config.Config); err != nil {
						return newSystemErrorWithCause(err, "setting Intel RDT config for ready process")
					}
				}

				if p.config.Config.Hooks != nil {
					s, err := p.container.currentOCIState()
					if err != nil {
						return err
					}
					// initProcessStartTime hasn't been set yet.
					s.Pid = p.cmd.Process.Pid
					s.Status = "creating"
					for i, hook := range p.config.Config.Hooks.Prestart {
						if err := hook.Run(s); err != nil {
							return newSystemErrorWithCausef(err, "running prestart hook %d", i)
						}
					}
				}
			}
			// Sync with child.
			if err := writeSync(p.messageSockPair.parent, procRun); err != nil {
				return newSystemErrorWithCause(err, "writing syncT 'run'")
			}
			sentRun = true
		case procHooks:
			// Setup cgroup before prestart hook, so that the prestart hook could apply cgroup permissions.
			if err := p.manager.Set(p.config.Config); err != nil {
				return newSystemErrorWithCause(err, "setting cgroup config for procHooks process")
			}
			if p.intelRdtManager != nil {
				if err := p.intelRdtManager.Set(p.config.Config); err != nil {
					return newSystemErrorWithCause(err, "setting Intel RDT config for procHooks process")
				}
			}
			if p.config.Config.Hooks != nil {
				s, err := p.container.currentOCIState()
				if err != nil {
					return err
				}
				// initProcessStartTime hasn't been set yet.
				s.Pid = p.cmd.Process.Pid
				s.Status = "creating"
				for i, hook := range p.config.Config.Hooks.Prestart {
					if err := hook.Run(s); err != nil {
						return newSystemErrorWithCausef(err, "running prestart hook %d", i)
					}
				}
			}
			// Sync with child.
			if err := writeSync(p.messageSockPair.parent, procResume); err != nil {
				return newSystemErrorWithCause(err, "writing syncT 'resume'")
			}
			sentResume = true
		default:
			return newSystemError(fmt.Errorf("invalid JSON payload from child"))
		}

		return nil
	})

	if !sentRun {
		return newSystemErrorWithCause(ierr, "container init")
	}
	if p.config.Config.Namespaces.Contains(configs.NEWNS) && !sentResume {
		return newSystemError(fmt.Errorf("could not synchronise after executing prestart hooks with container process"))
	}
	if err := unix.Shutdown(int(p.messageSockPair.parent.Fd()), unix.SHUT_WR); err != nil {
		return newSystemErrorWithCause(err, "shutting down init pipe")
	}

	// Must be done after Shutdown so the child will exit and we can wait for it.
	if ierr != nil {
		p.wait()
		return ierr
	}
	return nil
}
````

è¿è¡Œåˆ°è¿™é‡Œä¹Ÿå°±æ˜¯ `runc create` è¦è¿”å›äº†ï¼Œä½†æ˜¯å­è¿›ç¨‹çš„ `runc init` å› ä¸ºçˆ¶è¿›ç¨‹çš„é€€å‡ºè¢« `1` å·è¿›ç¨‹æ¥ç®¡ã€‚

## runc init

è¿™ä¸ªå°±æ˜¯ `contianer` å¯åŠ¨çš„æ—¶å€™`swap`å‰çš„è¿›ç¨‹

```go
// StartInitialization loads a container by opening the pipe fd from the parent to read the configuration and state
// This is a low level implementation detail of the reexec and should not be consumed externally
func (l *LinuxFactory) StartInitialization() (err error) {
	var (
		pipefd, fifofd int
		consoleSocket  *os.File
		envInitPipe    = os.Getenv("_LIBCONTAINER_INITPIPE")
		envFifoFd      = os.Getenv("_LIBCONTAINER_FIFOFD")
		envConsole     = os.Getenv("_LIBCONTAINER_CONSOLE")
	)

	// Get the INITPIPE.
	pipefd, err = strconv.Atoi(envInitPipe)
	if err != nil {
		return fmt.Errorf("unable to convert _LIBCONTAINER_INITPIPE=%s to int: %s", envInitPipe, err)
	}

	var (
		pipe = os.NewFile(uintptr(pipefd), "pipe")
		it   = initType(os.Getenv("_LIBCONTAINER_INITTYPE"))
	)
	defer pipe.Close()

	// Only init processes have FIFOFD.
	fifofd = -1
	if it == initStandard {
		if fifofd, err = strconv.Atoi(envFifoFd); err != nil {
			return fmt.Errorf("unable to convert _LIBCONTAINER_FIFOFD=%s to int: %s", envFifoFd, err)
		}
	}

	if envConsole != "" {
		console, err := strconv.Atoi(envConsole)
		if err != nil {
			return fmt.Errorf("unable to convert _LIBCONTAINER_CONSOLE=%s to int: %s", envConsole, err)
		}
		consoleSocket = os.NewFile(uintptr(console), "console-socket")
		defer consoleSocket.Close()
	}

	// clear the current process's environment to clean any libcontainer
	// specific env vars.
	os.Clearenv()

	defer func() {
		// We have an error during the initialization of the container's init,
		// send it back to the parent process in the form of an initError.
		if werr := utils.WriteJSON(pipe, syncT{procError}); werr != nil {
			fmt.Fprintln(os.Stderr, err)
			return
		}
		if werr := utils.WriteJSON(pipe, newSystemError(err)); werr != nil {
			fmt.Fprintln(os.Stderr, err)
			return
		}
	}()
	defer func() {
		if e := recover(); e != nil {
			err = fmt.Errorf("panic from initialization: %v, %v", e, string(debug.Stack()))
		}
	}()

	i, err := newContainerInit(it, pipe, consoleSocket, fifofd)
	if err != nil {
		return err
	}

	// If Init succeeds, syscall.Exec will not return, hence none of the defers will be called.
	return i.Init()
}
```

`newContainerInit()`æ•´ä½“é€»è¾‘ä¹Ÿæ¯”è¾ƒç®€å•

```go
func newContainerInit(t initType, pipe *os.File, consoleSocket *os.File, fifoFd int) (initer, error) {
	var config *initConfig
	if err := json.NewDecoder(pipe).Decode(&config); err != nil {
		return nil, err
	}
	if err := populateProcessEnvironment(config.Env); err != nil {
		return nil, err
	}
	switch t {
	case initSetns:
		return &linuxSetnsInit{
			pipe:          pipe,
			consoleSocket: consoleSocket,
			config:        config,
		}, nil
	case initStandard:
		return &linuxStandardInit{
			pipe:          pipe,
			consoleSocket: consoleSocket,
			parentPid:     unix.Getppid(),
			config:        config,
			fifoFd:        fifoFd,
		}, nil
	}
	return nil, fmt.Errorf("unknown init type %q", t)
}
```

å½“ä½ è¿è¡Œ `runc crate` è¿™ä¸ªæ—¶å€™çš„ `init` æ˜¯è°ƒç”¨å°±æ˜¯ `linuxStandardInit`

```go
func (l *linuxStandardInit) Init() error {
	runtime.LockOSThread()
	defer runtime.UnlockOSThread()
....	

	if err := setupNetwork(l.config); err != nil { // æ ¹æ® config ä½¿ç”¨ netlink è¿›è¡Œé…ç½®
		return err
	}
	if err := setupRoute(l.config.Config); err != nil { // ä½¿ç”¨ netlink è®¾ç½® route -
	// signal, so we restore it here.
	if err := pdeath.Restore(); err != nil {
		return errors.Wrap(err, "restore pdeath signal")
	}
	// Compare the parent from the initial start of the init process and make
	// sure that it did not change.  if the parent changes that means it died
	// and we were reparented to something else so we should just kill ourself
	// and not cause problems for someone else.
	if unix.Getppid() != l.parentPid {
		return unix.Kill(unix.Getpid(), unix.SIGKILL)
	}
	// Check for the arg before waiting to make sure it exists and it is
	// returned as a create time error.
	name, err := exec.LookPath(l.config.Args[0])
	if err != nil {
		return err
	}
	// Close the pipe to signal that we have completed our init.
	l.pipe.Close()
	// Wait for the FIFO to be opened on the other side before exec-ing the
	// user process. We open it through /proc/self/fd/$fd, because the fd that
	// was given to us was an O_PATH fd to the fifo itself. Linux allows us to
	// re-open an O_PATH fd through /proc.
  // çœ‹ä¸€ä¸‹æ³¨é‡Šï¼Œè¿™é‡Œåˆ©ç”¨äº† fifo çš„ç‰¹ç‚¹ï¼Œç­‰å¾… runc start æ¥å¼€è¿™ä¸ª fifo
	fd, err := unix.Open(fmt.Sprintf("/proc/self/fd/%d", l.fifoFd), unix.O_WRONLY|unix.O_CLOEXEC, 0) // ä¸€èµ·å‡†å¤‡å°±ç»ªè¿‡åä»–å°±ä¼š hang åœ¨è¿™é‡Œ
	if err != nil {
		return newSystemErrorWithCause(err, "open exec fifo")
	}
	if _, err := unix.Write(fd, []byte("0")); err != nil { // å½“ç”¨æˆ·è°ƒç”¨ runc start æ‰“å¼€ fifo ï¼Œå°±ä¼šæ‰§è¡Œåˆ°è¿™é‡Œ
		return newSystemErrorWithCause(err, "write 0 exec fifo")
	}
	// Close the O_PATH fifofd fd before exec because the kernel resets
	// dumpable in the wrong order. This has been fixed in newer kernels, but
	// we keep this to ensure CVE-2016-9962 doesn't re-emerge on older kernels.
	// N.B. the core issue itself (passing dirfds to the host filesystem) has
	// since been resolved.
	// https://github.com/torvalds/linux/blob/v4.9/fs/exec.c#L1290-L1318
	unix.Close(l.fifoFd)
	// Set seccomp as close to execve as possible, so as few syscalls take
	// place afterward (reducing the amount of syscalls that users need to
	// enable in their seccomp profiles).
	if l.config.Config.Seccomp != nil && l.config.NoNewPrivileges {
		if err := seccomp.InitSeccomp(l.config.Config.Seccomp); err != nil {
			return newSystemErrorWithCause(err, "init seccomp")
		}
	}
	if err := syscall.Exec(name, l.config.Args[0:], os.Environ()); err != nil { // è¿™é‡Œ swap ç”¨æˆ·çš„è¿›ç¨‹äº†
		return newSystemErrorWithCause(err, "exec user process")
	}
	return nil
}

```

## runc start 

å…¶å® `runc start` çš„é€»è¾‘æ›´ç®€å•ï¼Œä»…ä»…æ˜¯é€šè¿‡ `fifo` å’Œ `runc init`è¿›ç¨‹æ²Ÿé€šï¼Œè®©ä»–ç»§ç»­æ‰§è¡Œç”¨æˆ·è¿›ç¨‹ã€‚

```go
var startCommand = cli.Command{
	Name:  "start",
...
		container, err := getContainer(context)
		if err != nil {
			return err
		}
		status, err := container.Status()
		if err != nil {
			return err
		}
		switch status {
		case libcontainer.Created:
			return container.Exec()
```

```go
func (c *linuxContainer) Exec() error {
	c.m.Lock()
	defer c.m.Unlock()
	return c.exec()
}
```

æ‰“å¼€å­è¿›ç¨‹çš„`fifo.exec` æ–‡ä»¶ï¼Œå­è¿›ç¨‹å°±èƒ½ç»§ç»­æ‰§è¡Œä¸‹å»äº†ã€‚

```go
func (c *linuxContainer) exec() error {
	path := filepath.Join(c.root, execFifoFilename)
	pid := c.initProcess.pid()
	blockingFifoOpenCh := awaitFifoOpen(path)
	for {
		select {
		case result := <-blockingFifoOpenCh:
			return handleFifoResult(result)

		case <-time.After(time.Millisecond * 100):
			stat, err := system.Stat(pid)
			if err != nil || stat.State == system.Zombie {
				// could be because process started, ran, and completed between our 100ms timeout and our system.Stat() check.
				// see if the fifo exists and has data (with a non-blocking open, which will succeed if the writing process is complete).
				if err := handleFifoResult(fifoOpen(path, false)); err != nil {
					return errors.New("container process is already dead")
				}
				return nil
			}
		}
	}
}
```

```go
func handleFifoResult(result openResult) error {
	if result.err != nil {
		return result.err
	}
	f := result.file
	defer f.Close()
	if err := readFromExecFifo(f); err != nil { //
		return err
	}
	return os.Remove(f.Name())
}
```

```go
func readFromExecFifo(execFifo io.Reader) error {
	data, err := ioutil.ReadAll(execFifo)
	if err != nil {
		return err
	}
	if len(data) <= 0 {
		return fmt.Errorf("cannot start an already running container")
	}
	return nil
}
```

æ‰“å¼€ `fifo`ï¼Œå¦‚æœ `data` å°äº 0 è¯´æ˜è¿™ä¸ª  `fifo` é‡Œé¢ 0 å·²ç»è¢«è¯»å®Œäº†ï¼Œä¹Ÿå°±æ˜¯ running çš„ã€‚

## reference

[container-runtime-interface-cri-in-kubernetes/](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/)